\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{remark}{Remark}

\begin{document}

\title{Information, Codes and Ciphers}
\author{Hao Ren}
\date{\today}
\maketitle

\newpage

\tableofcontents

\newpage

\setcounter{section}{-1}

\section{All TODOs}

Updated before start the part \emph{3.4 Arithmetic Coding}.

\subsection{Unfinished Contents}

Finish this part at first and then add more items into \emph{0.2 Example and Diagrams} feature if needed.

\begin{itemize}
    \item 3.1.4 Decision Trees
    \item 3.2.4 Extensions of Huffman Coding
    \item 3.3.3 Huffman Coding for Stationary Markov Sources
\end{itemize}

\subsection{Examples and Diagrams}

\begin{itemize}
    \item 3.1.3 Comma Codes
    \item 3.2.1 Huffman Coding
    \item 3.2.3 Radix $r$ Huffman Codes
    \item 3.3.1 Definition of Markov Sources
\end{itemize}

\subsection{Proofs}

\begin{itemize}
    \item Theorem 3.1 The Kraft-McMillan Theorem
    \item Theorem 3.2 Minimal UD-codes
    \item Theorem 3.3 Huffman Code Theorem
    \item Proposition 3.4 Knuth
\end{itemize}

\setcounter{section}{2}

\newpage

\section{Compression Coding}

\subsection{Variable Length Encoding}

\subsubsection{Definition}

\begin{align*}
    \text{a source } S \qquad &\text{ with } q \text{ source symbols} \qquad & s_{1},s_{2},\cdots,s_{q},\\
    &\text{ with probabilities} \qquad &  p_{1},p_{2},\cdots,p_{q},\\
    \text{encoded by a code } C \qquad &\text{ with } q \text{ codewords} \qquad & c_{1},c_{2},\cdots,c_{q},\\
    &\text{ of lengths} \qquad & l_{1},l_{2},\cdots,l_{q}.
\end{align*}

\begin{itemize}
    \item with a radix $r$ codewords,
    \item variable length codes,
    \item not channel noise for source coding.
\end{itemize}

\subsubsection{UD and I-code}

A code $C$ is

\begin{description}
    \item[UD] uniquely decodable codes if it can always be decoded unambiguously,
    \item[I-code] instantaneous if no codeword is the prefix of others.
\end{description}

\subsubsection{Comma Codes}

The standard comma code of length $n$ is

\begin{itemize}
    \item a code which every codeword has length $\leq n$,
    \item a code which every codeword contains at most one $0$,
    \item and if a codeword contains $0$ then $0$ must be the final symbol in the codeword.
\end{itemize}

\subsubsection{Decision Trees}

\subsubsection{The Kraft-McMillan Theorem}

\begin{theorem}[The Kraft-McMillan Theorem]
    \mbox{}\\
    A UD-code of radix $r$ with $q$ codewords $c_{1}, c_{2},\cdots,c_{q}$ of lengths $l_{1} \leq l_{2} \leq \cdots \leq l_{q}$ exists \\
    if and only if \qquad an I-code with the same parameters exists \\
    if and only if
    \[K=\sum_{i=1}^{q}\frac{1}{r^{l_{i}}}\leq1.\]
\end{theorem}

\subsubsection{Length and Variance}

The expected or \textbf{average length} of codewords is given by
    \[L=\sum_{i=1}^{q}p_{i}l_{i}\]
    and the \textbf{variance} is given by
    \[V=\sum_{i=1}^{q}p_{i}l_{i}^{2}-L^{2}.\]

Our aim is to minimise $L$ for a given source $S$ and, if more than one code $C$ gives this value, to minimise $V$.

\begin{theorem}[Minimal UD-codes]
    \mbox{}\\
    Let $C$ be a UD-code with minimal expected length $L$ for the given source $S$. Then, after permuting codewords of equally likely symbols if necessary,
    \begin{itemize}
        \item $l_{1} \leq l_{2} \leq \cdots \leq l_{q}$ and
        \item $l_{q-1}=l_{1}$.
    \end{itemize}
    Furthermore, if C is instantaneous, then
    \begin{itemize}
        \item $c_{q-1}$ and $c_{q}$ differ only in their last place.
    \end{itemize}
    If $C$ is binary, then
    \begin{itemize}
        \item $K=\sum_{i=1}^{q}2^{-l_{i}}=1.$
    \end{itemize}
\end{theorem}

\subsection{Huffman's Algorithm}

\begin{quotation}
    Huffman’s algorithm for computing minimum-redundancy prefix-free codes has almost legendary status in the computing disciplines. Its elegant blend of simplicity and applicability has made it a favourite example in algorithms courses, and as a result, it is perhaps one of the most commonly implemented algorithmic techniques. \cite{Moffat_2019}
\end{quotation}

\subsubsection{Huffman Coding}

In 1952, David A. Huffman \cite{Huffman_1952} published a new lossless data compression method as an Sc.D student at MIT. Here is a demonstration to compute Huffman prefix-free code which is provided by Princeton University in the course COS226 \cite{princetonLec5.5}:

\begin{itemize}
    \item Count character frequencies $p_{s}$ for each symbol $s$ in file.
    \item Start with a forest of trees, each consisting of a single vertex corresponding to each symbol $s$ with weight $p_{s}$.
    \item Repeat:
        \begin{itemize}
            \item select two trees with min weight $p_{1}$ and $p_{2}$
            \item merge into single tree with weight $p_{1}+p_{2}$
        \end{itemize}
\end{itemize}

\paragraph{Applications} JPEG, MP3, MPEG, PKZIP.

\begin{theorem}[Huffman Code Theorem]
    \mbox{}\\
    For the given source $S$, the Huffman algorithm produces a minimum average length UD-code which is an instantaneous code.
\end{theorem}

\begin{proposition}[Knuth]
    \mbox{}\\
    For a Huffman code created by the given algorithm, the average code word length is sum of all the probabilities at child nodes.
\end{proposition}

\subsubsection{Properties of Huffman Codes}

\begin{enumerate}
    \item The place high strategy always produces a minimum variance Huffman code .
    \item If there are $2^{n}$ equally likely source symbols then the Huffman code is a block code of length $n$.
    \item If for all $j$, $3p_{j} \geq 2 \sum_{k=j+1}^{q}p_{k}$ then the Huffman code is a comma code.
    \item Small changes in the pi can change the Huffman code substantially, but have little effect on the average length $L$. This effect is smaller with smaller variance.
\end{enumerate}

\subsubsection{Radix $r$ Huffman Codes}

For $r$ radix Huffman codes, which $r \geq 3$, we have a better strategy by adding some dummy variables \cite{cmuLec8}.

For a $r$ radix encoding, the procedure is similar except $r$ least probable symbols are merged at each step. Since the total number of symbols may not be enough to allow $r$ variables to be merged at each step, we might need to add some dummy symbols with $0$ probability before constructing the Huffman tree.

How many dummy symbols need to be added? Since the first iteration merges $r$ symbols and then each iteration combines $r-1$ symbols with a merged symbols, if the procedure is to last for $k$ (some integer number of) iterations, then the total number of source symbols needed is $1+k(r-1)$. So before beginning the Huffman procedure, we add enough dummy symbols so that the total number of symbols look like $1+k(r-1)$ for the smallest possible value of $k$.

\begin{remark}
    \mbox{}
    \begin{enumerate}
        \item If more than two symbols have the same probability at any iteration, then the Huffman coding may not be unique (depending on the order in which they are merged). However, all Huffman codings on that alphabet are optimal in the sense they will yield the same expected code length.	
        \item 	One might think of another alternate procedure to assign small code lengths by building a tree top-down instead, e.g. divide the symbols into two sets with almost equal probabilities and repeating. While intuitively appealing, this procedure is suboptimal and leads to a larger expected code length than the Huffman encoding. You should try this on the symbol distribution described above.
    \end{enumerate}
\end{remark}

\subsubsection{Extensions of Huffman Coding}

\subsection{Markov Sources}

\subsubsection{Definition}

A finite-state Markov chain is a sequence $S_{0},S_{1}, \cdots$ of discrete random sym­bols from a finite alphabet, $S$. There is a probability mass function (pmf) $q_{0}(s), s \in S$ on $S_{0}$, and there is a conditional pmf $Q(s|s^{\prime})$ such that for all $m \geq 1$, all $s \in S$, and all $s^{\prime} \in S$,
    \[{\mathrm Pr}(S_{k}=s|S_{k-1}=s^{\prime})={\mathrm Pr}(S_{k}=s|S_{k-1}=s^{\prime},\cdots,S_{0}=s_{0})=Q(s|s^{\prime})\]
    There is said to be a \emph{transition} from $s^{\prime}$ to $s$, denoted $s^{\prime} \to S$, if $Q(s|s^{\prime})>0$. \cite{mitCh2}

\subsubsection{Transition Matrix}

The matrix $M = (p_{ij})$ is called the transition matrix of the Markov process, which could be displayed as $(\mathrm{from}\ s_{j}) \rightarrow (p_{ij}) \rightarrow (\mathrm{to}\ s_{i})$.

\begin{remark}
    \mbox{}
    \begin{itemize}
        \item \quad $P(s_{1}|s{j}) + P(s_{2}|s{j}) + \cdots + P(s_{q}|s{j}) = 1$
        \item \quad $p_{1j} + p_{2j} + \cdots + p_{qj} = 1, \; \mathrm{for}\ j = 1, \cdots, q$
    \end{itemize}
\end{remark}

\[
\begin{bmatrix}
    p_{11} & p_{12} & \cdots & p_{1j} \\
    p_{21} & p_{22} & \cdots & p_{2j} \\
    \vdots & \vdots & \ddots & \vdots \\
    p_{i1} & p_{i2} & \cdots & p_{ij}
\end{bmatrix}
\]

\subsubsection{Huffman Coding for Stationary Markov Sources}

\subsection{Arithmetic Coding}

\subsubsection{Encoding}

\subsubsection{Decoding}

\subsection{Dictionary Methods}

\subsubsection{Encoding}

\subsubsection{Decoding}

\subsection{Other Types of Compression}

\newpage

\bibliographystyle{IEEEtran}
\bibliography{refBibFile.bib}

\end{document}
